{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNwKeGVsyqXQ94dykrdEZ/D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harikang/yai_implementation/blob/harikang-patch-1/Transformer_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "aMFgy2FynFzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "Y2b-ZOLAnShJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding"
      ],
      "metadata": {
        "id": "VvR9Q9NWnLVE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of1_tc6cm6ZS"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "   def __init__(self, d_model, max_len, device):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # positional encoding은 input embedding과 elementwise더해져야한다.\n",
        "        # encoding = [단어의 개수 x 단어 별 차원]\n",
        "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
        "        self.encoding.requires_grad = False  # we don't need to compute gradient\n",
        "\n",
        "        pos = torch.arange(0, max_len, device=device)#0부터 max_length까지 1씩 증가하여 쌓임\n",
        "        pos = pos.float().unsqueeze(dim=1) # pos = [max_length x 1]차원\n",
        "       #0부터 d_model까지 2 간격으로 순서대로 쌓임\n",
        "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
        "        # 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])\n",
        "        # \"step=2\" means 'i' multiplied with two (same with 2 * i)\n",
        "        #아래 코드가 PE 코드를 의미하며 PE[단어의 개수 X 단어의 차원]\n",
        "        #단어 별 FEATURE가 짝수일 때,\n",
        "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
        "        #단어 별 FEATURE가 홀수일 때,\n",
        "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
        "        # compute positional encoding to consider positional information of words\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size()\n",
        "        # [batch_size = 문장의 개수, seq_len = 문장 별 단어의 최대 개수]\n",
        "        return self.encoding[:seq_len, :]\n",
        "        # [max_len, 단어의 차원]\n",
        "        #단어의 특징을 나타내는 차원마다 주파수가 고정된다.\n",
        "        #단어의 순서에 따라 pos값이 변하면서 다른 값으로 지정된다.\n",
        "        #즉, 다른 문장의 같은 위치의 특정 차원은 같은 값을 가짐"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input Embedding"
      ],
      "metadata": {
        "id": "xEw96Ti4nVdA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TEOQuh_0nFJQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}